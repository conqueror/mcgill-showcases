# Responsible AI Track

This track combines fairness diagnostics, explainability methods, and causal reasoning for safer model-driven decisions.

## Recommended Sequence

1. `projects/xai-fairness-audit-showcase`
2. `projects/causalml-kaggle-showcase`
3. `projects/sota-supervised-learning-showcase` (evaluation extension)

## Core Skills Covered

- Explainability with SHAP/LIME.
- Subgroup fairness analysis and mitigation tradeoffs.
- Causal treatment effect estimation and policy simulation.
- Interpretation of model decisions under uncertainty.

## Evidence Artifacts To Inspect

- explainability outputs in `artifacts/explainability/`
- subgroup fairness reports in XAI/fairness artifacts
- uplift and policy simulation outputs in causal showcase

## Suggested Reflection Prompts

- When is an explainable model still unsafe to deploy?
- Which subgroup metric is most meaningful for this use case?
- How does causal uplift change action policy compared with pure prediction?
