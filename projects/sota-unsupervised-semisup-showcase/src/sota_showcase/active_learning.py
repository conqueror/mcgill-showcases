"""Active learning simulation for label-efficient classification."""

from __future__ import annotations

from dataclasses import dataclass

import numpy as np
import pandas as pd
from numpy.typing import NDArray
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score

ArrayF = NDArray[np.float64]
ArrayI = NDArray[np.int64]


@dataclass(frozen=True)
class ActiveLearningOutput:
    """Tabular metrics generated by active learning runs."""

    metrics: pd.DataFrame


def _build_classifier(random_state: int) -> LogisticRegression:
    return LogisticRegression(
        max_iter=1_500,
        random_state=random_state,
    )


def _ensure_at_least_two_classes(
    labeled_mask: NDArray[np.bool_],
    y_true: ArrayI,
    rng: np.random.Generator,
) -> NDArray[np.bool_]:
    """Guarantee trainability when initial labeled subset misses a class."""

    updated = labeled_mask.copy()
    if np.unique(y_true[updated]).size >= 2:
        return updated

    unlabeled_indices = np.flatnonzero(~updated)
    for idx in rng.permutation(unlabeled_indices):
        updated[idx] = True
        if np.unique(y_true[updated]).size >= 2:
            break

    return updated


def _select_uncertain_indices(
    model: LogisticRegression,
    X_pool: ArrayF,
    pool_indices: ArrayI,
    query_size: int,
) -> ArrayI:
    probabilities = model.predict_proba(X_pool)
    if probabilities.shape[1] == 2:
        uncertainty = np.abs(probabilities[:, 1] - 0.5)
        order = np.argsort(uncertainty)
    else:
        ranked = np.sort(probabilities, axis=1)
        margin = ranked[:, -1] - ranked[:, -2]
        order = np.argsort(margin)

    chosen = pool_indices[order[:query_size]]
    return chosen.astype(np.int64)


def _run_one_strategy(
    strategy: str,
    X_train: ArrayF,
    y_train: ArrayI,
    X_test: ArrayF,
    y_test: ArrayI,
    initial_labeled_mask: NDArray[np.bool_],
    rounds: int,
    query_size: int,
    random_state: int,
) -> list[dict[str, float | int | str]]:
    rng = np.random.default_rng(random_state)
    labeled_mask = _ensure_at_least_two_classes(initial_labeled_mask, y_train, rng)

    rows: list[dict[str, float | int | str]] = []
    for round_idx in range(rounds + 1):
        if np.unique(y_train[labeled_mask]).size < 2:
            break

        model = _build_classifier(random_state=random_state + round_idx)
        model.fit(X_train[labeled_mask], y_train[labeled_mask])

        predictions = model.predict(X_test)
        rows.append(
            {
                "strategy": strategy,
                "round": round_idx,
                "labeled_budget": int(np.sum(labeled_mask)),
                "accuracy": float(accuracy_score(y_test, predictions)),
                "f1_macro": float(f1_score(y_test, predictions, average="macro")),
            },
        )

        pool_indices = np.flatnonzero(~labeled_mask)
        if round_idx == rounds or pool_indices.size == 0:
            continue

        step_size = min(query_size, pool_indices.size)
        if strategy == "uncertainty":
            chosen_indices = _select_uncertain_indices(
                model=model,
                X_pool=X_train[pool_indices],
                pool_indices=pool_indices.astype(np.int64),
                query_size=step_size,
            )
        else:
            chosen_indices = rng.choice(
                pool_indices,
                size=step_size,
                replace=False,
            ).astype(np.int64)

        labeled_mask[chosen_indices] = True

    return rows


def run_active_learning_simulation(
    X_train: ArrayF,
    y_train: ArrayI,
    y_train_masked: ArrayI,
    X_test: ArrayF,
    y_test: ArrayI,
    random_state: int,
    rounds: int,
    query_size: int,
) -> ActiveLearningOutput:
    """Compare uncertainty sampling against random sampling under equal label budgets."""

    initial_labeled_mask = y_train_masked != -1

    rows: list[dict[str, float | int | str]] = []
    rows.extend(
        _run_one_strategy(
            strategy="random",
            X_train=X_train,
            y_train=y_train,
            X_test=X_test,
            y_test=y_test,
            initial_labeled_mask=initial_labeled_mask,
            rounds=rounds,
            query_size=query_size,
            random_state=random_state,
        ),
    )
    rows.extend(
        _run_one_strategy(
            strategy="uncertainty",
            X_train=X_train,
            y_train=y_train,
            X_test=X_test,
            y_test=y_test,
            initial_labeled_mask=initial_labeled_mask,
            rounds=rounds,
            query_size=query_size,
            random_state=random_state + 1,
        ),
    )

    metrics = pd.DataFrame(rows).sort_values(["strategy", "round"]).reset_index(drop=True)
    return ActiveLearningOutput(metrics=metrics)
