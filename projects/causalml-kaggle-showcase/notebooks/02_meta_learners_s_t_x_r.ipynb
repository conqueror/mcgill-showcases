{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tutorial: Causal Inference 02 - Meta-Learners S T X R\n",
        "\n",
        "Audience:\n",
        "- Students ready to move from ATE to heterogeneous treatment effects.\n",
        "\n",
        "Prerequisites:\n",
        "- Notebook 01.\n",
        "- Understanding of train/test splits.\n",
        "\n",
        "Learning goals:\n",
        "- Train S/T/X/R learners with CausalML.\n",
        "- Compare models using policy-facing metrics.\n",
        "- Interpret why model rankings can differ.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outline\n",
        "\n",
        "1. Data split and model training.\n",
        "2. Compare ATE estimates and uplift@30%.\n",
        "3. Inspect uplift score distributions.\n",
        "4. Exercise + pitfall + extension.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "project_root = Path.cwd().resolve()\n",
        "if not (project_root / \"src\").exists():\n",
        "    project_root = project_root.parent\n",
        "\n",
        "sys.path.insert(0, str(project_root / \"src\"))\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from causal_showcase.data import load_marketing_ab_data, train_test_split_prepared\n",
        "from causal_showcase.evaluation import uplift_at_k\n",
        "from causal_showcase.modeling import fit_meta_learners\n",
        "\n",
        "data_path = project_root / \"data\" / \"raw\" / \"marketing_ab.csv\"\n",
        "prepared = load_marketing_ab_data(data_path)\n",
        "train_data, test_data = train_test_split_prepared(prepared)\n",
        "\n",
        "results = fit_meta_learners(train_data, test_data)\n",
        "print(\"Learners:\", list(results.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 - Metric table across learners\n",
        "\n",
        "We compare each learner on:\n",
        "- `estimated_ate` (global effect estimate),\n",
        "- `uplift_at_30pct` (policy-relevant targeting quality).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = []\n",
        "for name, result in results.items():\n",
        "    rows.append(\n",
        "        {\n",
        "            \"model\": name,\n",
        "            \"estimated_ate\": result.ate,\n",
        "            \"ate_ci_low\": result.ate_ci_low,\n",
        "            \"ate_ci_high\": result.ate_ci_high,\n",
        "            \"uplift_at_30pct\": uplift_at_k(\n",
        "                test_data.outcome,\n",
        "                test_data.treatment,\n",
        "                result.uplift_scores,\n",
        "                top_fraction=0.30,\n",
        "            ),\n",
        "        }\n",
        "    )\n",
        "\n",
        "metrics_df = pd.DataFrame(rows).sort_values(\"uplift_at_30pct\", ascending=False)\n",
        "metrics_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 - Uplift score quantiles\n",
        "\n",
        "Quantiles help us see whether a learner spreads users into distinct low/high uplift groups.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quantile_rows = []\n",
        "for name, result in results.items():\n",
        "    q = pd.Series(result.uplift_scores).quantile([0.1, 0.5, 0.9]).to_dict()\n",
        "    quantile_rows.append(\n",
        "        {\n",
        "            \"model\": name,\n",
        "            \"q10\": q[0.1],\n",
        "            \"q50\": q[0.5],\n",
        "            \"q90\": q[0.9],\n",
        "        }\n",
        "    )\n",
        "\n",
        "pd.DataFrame(quantile_rows).sort_values(\"q90\", ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises, pitfalls, and extension\n",
        "\n",
        "- Exercise: Evaluate `uplift_at_20pct` and compare rank changes.\n",
        "- Pitfall: Interpreting ATE alone as policy quality.\n",
        "- Extension: Swap the base learner and compare robustness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ranking_at_fraction(fraction: float) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for name, result in results.items():\n",
        "        rows.append(\n",
        "            {\n",
        "                \"model\": name,\n",
        "                \"uplift_at_fraction\": uplift_at_k(\n",
        "                    test_data.outcome,\n",
        "                    test_data.treatment,\n",
        "                    result.uplift_scores,\n",
        "                    top_fraction=fraction,\n",
        "                ),\n",
        "            }\n",
        "        )\n",
        "    return pd.DataFrame(rows).sort_values(\"uplift_at_fraction\", ascending=False)\n",
        "\n",
        "ranking_at_fraction(0.20)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
